{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Intro to Generalized Stability Theory\n",
    "\n",
    "\n",
    "#### Navid C. Constantinou\n",
    "#### RSES, ANU, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear Systems\n",
    "\n",
    "Consider the linear system that describes perturbations about a basic state,\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\mathrm{d} \\boldsymbol{\\phi}}{\\mathrm{d}t}  = \\mathbb{A}\\,\\boldsymbol{\\phi},\\quad \\boldsymbol{\\phi}(t_0)=\\boldsymbol{\\phi}_0. \\tag{1}\n",
    "\\end{align}\n",
    "\n",
    "Above, $\\boldsymbol{\\phi}$ is the state vector that collectively describes the perturbations about the basic state and  $\\mathbb{A}$ is a linear operator (which implicitly depends on the basic state).\n",
    "\n",
    "We are interested on how the energy of the perturbations is going to evolves with time.\n",
    "\n",
    "Energy is a quadratic quantity usually defined through an inner product. Let's begin for simplicity with the case in which the perturbation energy is given through an inner product\n",
    "\n",
    "\\begin{align}\n",
    "E(t) = \\big(\\boldsymbol{\\phi}(t), \\boldsymbol{\\phi}(t)\\big). \\tag{2}\n",
    "\\end{align}\n",
    "\n",
    "For example, for a finite-dimensional state space, e.g., $\\mathbb{C}^{n\\times 1}$, the inner product can be simply the Euclidean inner product \n",
    "\n",
    "\\begin{align}\n",
    "(\\boldsymbol{\\psi}, \\boldsymbol{\\phi}) \\equiv \\boldsymbol{\\psi}^\\dagger \\boldsymbol{\\phi} = \\sum_{j=1}^n \\psi_j^* \\phi_j. \\tag{3}\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The linear system (1) has solution:\n",
    "\n",
    "\\begin{align}\n",
    "\\boldsymbol{\\phi}(t) = \\mathbb{P}(t,t_0)\\,\\boldsymbol{\\phi}_0 , \\tag{4}\n",
    "\\end{align}\n",
    "\n",
    "where $\\Phi(t,t_0)$ is a *linear* operator that maps the state at time $t_0$ to the state at time $t$. Linear map $\\Phi$ is called the *propagator*. \n",
    "\n",
    "When the linear operator in (1) is time-independent then the propagator is nothing else than\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{P}(t,t_0) = \\exp{(\\mathbb{A}(t-t_0))} , \\tag{5}\n",
    "\\end{align}\n",
    "\n",
    "defined as\n",
    "\n",
    "\\begin{align}\n",
    "\\exp{\\big(\\mathbb{A}(t-t_0)\\big)} = \\sum_{n=0}^{\\infty} \\frac{\\mathbb{A}^n(t-t_0)^n}{n!} . \\tag{6}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "**Exercise**: Verify that expression (4) with $\\mathbb{P}$ given by (5) solves (1).\n",
    "\n",
    "**Exercise**: Verify that (6) satifies property (4). (Just convince yourself that the first few terms in the power series, e.g., up to the term $\\mathcal{O}(\\mathbb{A}^2)$, *do* follow this property.)\n",
    "\n",
    "\n",
    "From hereafter let's take $t_0=0$ for simplicity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Definition 1**: The adjoint operator of $\\mathbb{A}$ is denoted as $\\mathbb{A}^\\dagger$ and is defined through:\n",
    "\n",
    "\\begin{align}\n",
    "(\\boldsymbol{\\psi}, \\mathbb{A}\\boldsymbol{\\phi}) = (\\mathbb{A}^\\dagger\\boldsymbol{\\psi}, \\boldsymbol{\\phi}) ,\\quad\\mathit{for}\\text{ }\\mathit{every}\\text{ }\\boldsymbol{\\phi},\\boldsymbol{\\psi} \\text{ in the vector space}. \\tag{8}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "**Definition 2 (Adjoint operator)**: An operator $\\mathbb{A}$ is called *self-adjoint* or *Hermitian* if and only if $\\mathbb{A}=\\mathbb{A}^\\dagger$.\n",
    "\n",
    "**Definition 3 (Self-adjoint operator)**: An operator $\\mathbb{A}$ is called *normal* if and only if $\\mathbb{A}\\mathbb{A}^\\dagger = \\mathbb{A}^\\dagger \\mathbb{A}$.\n",
    "\n",
    "**Corrolary**: Self-adjoint operators are necessarily normal.\n",
    "\n",
    "**Theorem**: Self-adjoint operators have real eigenvalues.\n",
    "\n",
    "*Proof*: If $\\lambda$ is an eigenvalue of $\\mathbb{A}$ and $\\boldsymbol{u}$ the corresponding eigenvector, $\\mathbb{A}\\,\\boldsymbol{u} = \\lambda\\,\\boldsymbol{u}$. By forming:\n",
    "\n",
    "\\begin{align*}\n",
    "(\\boldsymbol{u}, \\mathbb{A}\\boldsymbol{u}) = (\\boldsymbol{u}, \\lambda \\boldsymbol{u}) = \\lambda (\\boldsymbol{u}, \\boldsymbol{u}).\n",
    "\\end{align*}\n",
    "\n",
    "On the other hand, using that $\\mathbb{A} = \\mathbb{A}^\\dagger$ we have\n",
    "\n",
    "\\begin{align*}\n",
    "(\\boldsymbol{u}, \\mathbb{A}\\boldsymbol{u}) = (\\mathbb{A}^\\dagger \\boldsymbol{u}, \\boldsymbol{u}) = (\\mathbb{A} \\boldsymbol{u}, \\boldsymbol{u}) = (\\lambda \\boldsymbol{u}, \\boldsymbol{u}) =\\lambda^* (\\boldsymbol{u}, \\boldsymbol{u}).\n",
    "\\end{align*}\n",
    "\n",
    "From these we get that $(\\lambda-\\lambda^*)(\\boldsymbol{u}, \\boldsymbol{u})=0$ and since $|\\boldsymbol{u}|>0$ we get that $\\lambda=\\lambda^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "using LinearAlgebra, Printf, PyPlot\n",
    "Base.show(io::IO, f::Float64) = @printf(io, \"%1.2f\", f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A=\n",
      "2×2 Array{Float64,2}:\n",
      " -1.50  -0.50\n",
      " -0.50  -1.50 \n",
      " \n",
      "norm(A-A') = 0.00, eigvalues of A = [-2.00, -1.00]\n",
      " \n",
      "A=\n",
      "2×2 Array{Int64,2}:\n",
      " -1  1\n",
      "  0  1 \n",
      " \n",
      "norm(A-A') = 1.41, eigvalues of A = [-1.00, 1.00]\n",
      " \n",
      "A=\n",
      "2×2 Array{Complex{Float64},2}:\n",
      " 0.88+0.00im  0.50+0.12im\n",
      " 0.50-0.12im  0.61+0.00im \n",
      " \n",
      "norm(A-A') = 0.00, eigvalues of A = [0.21, 1.28]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "function analyze_operator(A)\n",
    "    S, U = eigen(A);\n",
    "    println(\"A=\"); show(stdout, \"text/plain\", A);\n",
    "    println(\" \"); println(\" \")\n",
    "    println(\"norm(A-A') = \", norm(A-A'), \", eigvalues of A = \", S);\n",
    "    println(\" \");\n",
    "end\n",
    "\n",
    "A = [-3/2 -1/2; -1/2 -3/2]; analyze_operator(A)\n",
    "A = [-1 1; 0 1]; analyze_operator(A)\n",
    "A = rand(2, 2) + im*rand(2,2); A = (A+A')/2; # this is to convert the random A to a self-adjoint one\n",
    "analyze_operator(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Definition 4 (Rayleigh's quotient)**: For any self-adjoint operator $\\mathbb{M}$ the quantity:\n",
    "\n",
    "\\begin{align}\n",
    "R(\\mathbb{M}, \\boldsymbol{\\phi}) = \\frac{(\\boldsymbol{\\phi}, \\mathbb{M} \\boldsymbol{\\phi})}{(\\boldsymbol{\\phi}, \\boldsymbol{\\phi})}, \\tag{9}\n",
    "\\end{align}\n",
    "\n",
    "is called *Rayleigh's quotient*.\n",
    "\n",
    "**Properties of Rayleigh's quotient**:\n",
    "\n",
    "Rayleigh's quotient is a function that maps vectors $\\boldsymbol{\\phi}$ to scalars. It satisfies the following properties:\n",
    "\n",
    "1. $R(\\mathbb{M}, \\boldsymbol{u})$ is real for *any* $\\boldsymbol{\\phi}$ in the vector space.\n",
    "2. $R(\\mathbb{M}, c\\boldsymbol{\\phi}) = R(\\boldsymbol{\\phi})$.\n",
    "3. $\\lambda_{\\min}\\le R(\\mathbb{M}, \\boldsymbol{\\phi})\\le \\lambda_{\\max}$, where is $\\lambda_{\\min},\\lambda_{\\max}$ are the minimum and maximum eigenvalues of $\\mathbb{M}$.\n",
    "4. The stationary values of $R$ are the eigenvalues of $\\mathbb{M}$ and they are obtained when $\\boldsymbol{\\phi}$ is the corresponding eigenvector of $\\mathbb{M}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Exercise**: Show property 1 of Rayleigh's quotient.\n",
    "\n",
    "**Exercise** (bit harder): Show property 4 of Rayleigh's quotient. That is, assume that $R(\\mathbb{M}, \\boldsymbol{\\phi})$ is a stationary point and show that $\\boldsymbol{\\phi}$ is necessarily an eigenvector of $\\mathbb{M}$. \n",
    "\n",
    "Hint: To do so, write consider the Rayleigh's quotient evaluated at the slightly perturbed state $R(\\mathbb{M}, \\boldsymbol{\\phi}+\\delta\\boldsymbol{\\phi})$ with $\\|\\delta\\boldsymbol{\\phi}\\|\\ll\\|\\boldsymbol{\\phi}\\|$. Since $R(\\mathbb{M}, \\boldsymbol{\\phi})$ is a stationary point then it must be that\n",
    "\n",
    "\\begin{align*}\n",
    "R(\\mathbb{M}, \\boldsymbol{\\phi}+\\delta\\boldsymbol{\\phi}) - R(\\mathbb{M}, \\boldsymbol{\\phi}) = \\mathcal{O}\\big[(\\delta\\boldsymbol{\\phi})^2\\big].\n",
    "\\end{align*}\n",
    "\n",
    "From properties 3 and 4 it is evident that the maximum (or minimum) value of Rayleigh's quotient $R$ is obtained for the  eigenvector of $\\mathbb{M}$ that corresponds to $\\lambda_{\\max}$ (or $\\lambda_{\\min}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Theorem (Properties of the adjoint)**: The adjoint operator $\\mathbb{A}^\\dagger$ has (i) eigenvalues which are complex conjugates of the eigenvalues of $\\mathbb{A}$. Furthermore, (ii) the eigenvectors of $\\mathbb{A}$ and $\\mathbb{A}^\\dagger$ that *do not* correspond to a complex conjugate eigenvalue pair are orthogonal to each other.\n",
    "\n",
    "*Proof*:\n",
    "\n",
    "(i) The eigenvalues $\\lambda$ of $\\mathbb{A}$ solve the characteristic polynomial $\\mathrm{det}(\\mathbb{A}-\\lambda\\mathbb{I})=0$. Using that $\\mathrm{det}{(\\mathbb{A}^\\dagger)} = \\mathrm{det}{(\\mathbb{A})}^*$ we can show that\n",
    "\n",
    "\\begin{align}\n",
    "0 = \\mathrm{det}(\\mathbb{A}-\\lambda\\mathbb{I})^*=\\mathrm{det}\\big[(\\mathbb{A}-\\lambda\\mathbb{I})^\\dagger\\big]=\\mathrm{det}(\\mathbb{A}^\\dagger-\\lambda^*\\mathbb{I}),\n",
    "\\end{align}\n",
    "\n",
    "which implies that the eigenvalues of $\\mathbb{A}^\\dagger$ are $\\lambda^*$, i.e., the complex conjugates of the eigenvalues of $\\mathbb{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "(ii) Consider now the eigenvector/eigenvalue relations for $\\mathbb{A}$ and $\\mathbb{A}^\\dagger$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{A}\\,\\boldsymbol{u}_j = \\lambda_j\\,\\boldsymbol{u}_j\\quad\\text{and}\\quad \n",
    "\\mathbb{A}^\\dagger\\boldsymbol{\\upsilon}_j = \\lambda_j^*\\,\\boldsymbol{\\upsilon}_j. \\tag{10a,b}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Take for simplicity that there is no degeneracy. By taking the inner product of (10a) with the eigenvector $\\boldsymbol{\\upsilon}_i$ we have\n",
    "\n",
    "\\begin{align*}\n",
    "(\\boldsymbol{\\upsilon}_i, \\mathbb{A}\\boldsymbol{u}_j) &= (\\boldsymbol{\\upsilon}_i, \\lambda_j\\boldsymbol{u}_j) = \\lambda_j\\,(\\boldsymbol{\\upsilon}_i,\\boldsymbol{u}_j).\n",
    "\\end{align*}\n",
    "\n",
    "But on the other hand, using the definition of $\\mathbb{A}^\\dagger$, \n",
    "\n",
    "\\begin{align*}\n",
    "(\\boldsymbol{\\upsilon}_i, \\mathbb{A}\\boldsymbol{u}_j) &= (\\mathbb{A}^\\dagger\\boldsymbol{\\upsilon}_i, \\boldsymbol{u}_j) = (\\lambda_i^*\\boldsymbol{\\upsilon}_i, \\boldsymbol{u}_j) = \\lambda_i\\,(\\boldsymbol{\\upsilon}_i,\\boldsymbol{u}_j).\n",
    "\\end{align*}\n",
    "\n",
    "These imply that\n",
    "\n",
    "\\begin{align*}\n",
    "(\\lambda_i-\\lambda_j)\\,(\\boldsymbol{\\upsilon}_i,\\boldsymbol{u}_j) = 0,\n",
    "\\end{align*}\n",
    "\n",
    "and since there is no degeneracy (i.e., $\\lambda_i\\ne\\lambda_j$) it follows that\n",
    "\n",
    "\\begin{align*}\n",
    "(\\boldsymbol{\\upsilon}_i,\\boldsymbol{u}_j) = 0\\text{ for }i\\ne j.\\tag{11}\n",
    "\\end{align*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A=\n",
      "2×2 Array{Complex{Float64},2}:\n",
      " 0.81+0.80im  0.65+0.82im\n",
      " 0.45+0.50im  0.64+0.34im \n",
      " \n",
      "eigvalues of A: λ1=1.27 + 1.25im , λ2=0.18 - 0.11im\n",
      " \n",
      "A'=\n",
      "2×2 Adjoint{Complex{Float64},Array{Complex{Float64},2}}:\n",
      " 0.81-0.80im  0.45-0.50im\n",
      " 0.65-0.82im  0.64-0.34im \n",
      " \n",
      "eigvalues of A': μ1=1.27 - 1.25im , μ2=0.18 + 0.11im\n",
      " \n",
      "inner products between eigenvectors of A and of A'\n",
      "(u1, v1) = 0.97 + 0.07im, (u1, v2) = 0.00 - 0.00im\n",
      "(u2, v1) = 0.00 + 0.00im, (u2, v2) = 0.97 + 0.07im\n"
     ]
    }
   ],
   "source": [
    "A = rand(2, 2) + im*rand(2,2); \n",
    "\n",
    "S, U = eigen(A); u1 = U[:, 1]; u2 =U[:, 2]\n",
    "Sadj, V = eigen(copy(A')); v1 = V[:, 1]; v2 =V[:, 2]\n",
    "\n",
    "println(\"A=\"); show(stdout, \"text/plain\", A);\n",
    "println(\" \"); println(\" \")\n",
    "println(\"eigvalues of A: λ1=\", S[1], \" , λ2=\", S[2]); println(\" \"); \n",
    "\n",
    "println(\"A'=\"); show(stdout, \"text/plain\", A');\n",
    "println(\" \"); println(\" \")\n",
    "println(\"eigvalues of A': μ1=\", Sadj[1], \" , μ2=\", Sadj[2]); println(\" \");\n",
    "\n",
    "println(\"inner products between eigenvectors of A and of A'\");\n",
    "println(\"(u1, v1) = \", dot(u1, v1), \", (u1, v2) = \", dot(u1, v2))\n",
    "println(\"(u2, v1) = \", dot(u2, v1), \", (u2, v2) = \", dot(u2, v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we have all tools available to attack the question of maximum energy growth for linear system (1).\n",
    "\n",
    "### Energy growth\n",
    "\n",
    "The energy growth for linear system (1) is $E(t)\\big/E(0)$. Using solution (4)-(5) we get\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{E(t)}{E(0)} &= \\frac{\\big(e^{\\mathbb{A}t}\\boldsymbol{u}_0\\,,\\,e^{\\mathbb{A}t}\\boldsymbol{u}_0\\big)}{\\big(\\boldsymbol{u}_0\\,,\\,\\boldsymbol{u}_0\\big)}\\\\\n",
    "&= \\frac{\\big(\\boldsymbol{u}_0\\,,\\,(e^{\\mathbb{A}t})^\\dagger e^{\\mathbb{A}t}\\boldsymbol{u}_0\\big)}{\\big(\\boldsymbol{u}_0\\,,\\,\\boldsymbol{u}_0\\big)}\\\\\n",
    "&= \\frac{\\big(\\boldsymbol{u}_0\\,,\\,e^{\\mathbb{A^\\dagger}t} e^{\\mathbb{A}t}\\boldsymbol{u}_0\\big)}{\\big(\\boldsymbol{u}_0\\,,\\,\\boldsymbol{u}_0\\big)}.\\tag{12}\n",
    "\\end{align*}\n",
    "\n",
    "But that's nothing else than the Rayleigh quotient of operator $e^{\\mathbb{A^\\dagger}t} e^{\\mathbb{A}t}$!\n",
    "\n",
    "**Exercise**: Show that operator $e^{\\mathbb{A^\\dagger}t} e^{\\mathbb{A}t}$ is self-adjoint and, furthermore, that its eigenvalues are positive.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Theorem (bounds on perturbation energy growth)**\n",
    "\n",
    "The energy growth $E(t)\\big/E(0)$ of linear system (1) is bounded from above by the maximum eigenvalue of $e^{\\mathbb{A^\\dagger}t} e^{\\mathbb{A}t}$. The inital state $\\boldsymbol{u}_0$ that produces maximum energy growth at time $t$ is the eigenvector of $e^{\\mathbb{A^\\dagger}t} e^{\\mathbb{A}t}$ that corresponds to its maximum eigenvalue.\n",
    "\n",
    "Similarly, $E(t)\\big/E(0)$ is bounded from below by the minimum eigenvalue of $e^{\\mathbb{A^\\dagger}t} e^{\\mathbb{A}t}$ with the corresponding eigenvector being the initial condition that produces this minimum energy growth.\n",
    "\n",
    "*Proof*: This follows after the remark that $E(t)/E(0) = R\\big(e^{\\mathbb{A^\\dagger}t} e^{\\mathbb{A}t}\\,,\\, \\boldsymbol{u}_0\\big)$ and employing the properties of the Rayleigh quotient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Corrolary 1**\n",
    "\n",
    "For $t\\ll1$ all eigenvectors of $e^{\\mathbb{A^\\dagger}t} e^{\\mathbb{A}t}$ coincide with the eigenvectors of $\\mathbb{A^\\dagger} + \\mathbb{A}$.\n",
    "\n",
    "**Corrolary 2**\n",
    "\n",
    "For $t \\to \\infty$ the eigenvector of $e^{\\mathbb{A^\\dagger}t} e^{\\mathbb{A}t}$ that corresponds to the largest eigenvalue coincides with the eigenvector of $\\mathbb{A^\\dagger}$ that corresponds to the eigenvalue with maximum real part.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The theorem on bounds on perturbation energy growth is general and holds for both normal and non-normal operators.\n",
    "\n",
    "For normal operators we have that operators: $\\mathbb{A}$, $\\mathbb{A}$, $(\\mathbb{A}+\\mathbb{A}^\\dagger)$, and $e^{\\mathbb{A}^\\dagger t}e^{\\mathbb{A}t}$ *all* commute with each other and thus share the same eigenvectors. Therefore one needs not need to distinguish between them; only computing the eigenvectors of $\\mathbb{A}$ is enough to give you everything you need. Thus, for normal operators the notion of perturbation growth is *equivalent* with the notion of modal instability. However, this is not the case at all for non-normal operators. The theorem above generalizes the notion of modal instability to take into account non-normality and transient energy growth effects. It lies in the heart of the, so called, Generalized Stability Theory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Proof of Corrolary 1**\n",
    "\n",
    "This is so because\n",
    "\\begin{align*}\n",
    "\\frac{E(t)}{E(0)} &= \\frac{\\big(\\boldsymbol{u}_0\\,,\\,e^{\\mathbb{A^\\dagger}t} e^{\\mathbb{A}t}\\boldsymbol{u}_0\\big)}{\\big(\\boldsymbol{u}_0\\,,\\,\\boldsymbol{u}_0\\big)}\\\\\n",
    "&\\approx \\frac{\\big(\\boldsymbol{u}_0\\,,[\\mathbb{I} + \\mathbb{A^\\dagger}t + \\mathcal{O}(t^2)][\\mathbb{I} + \\mathbb{A}t + \\mathcal{O}(t^2)]\\boldsymbol{u}_0\\big)}{\\big(\\boldsymbol{u}_0\\,,\\,\\boldsymbol{u}_0\\big)}\\\\\n",
    "&= 1 + t\\frac{\\big(\\boldsymbol{u}_0\\,,(\\mathbb{A^\\dagger}+\\mathbb{A})\\boldsymbol{u}_0\\big)}{\\big(\\boldsymbol{u}_0\\,,\\,\\boldsymbol{u}_0\\big)} + \\mathcal{O}(t^2),\\tag{13}\n",
    "\\end{align*}\n",
    "which is nothing else than the Rayleigh quotient of $\\mathbb{A^\\dagger}+\\mathbb{A}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Proof of Corrolary 2**\n",
    "\n",
    "This is a bit harder to see. We will show it here for the case of finite-dimensional operators.\n",
    "\n",
    "First let's make a remark that about the eigenvectors of the adjoint $\\mathbb{A}^\\dagger$. If $\\mathbb{U}$ is the matrix whose columns are the eigenvectors $\\boldsymbol{u}_j$ of $\\mathbb{A}$ then we can get the matrix $\\widetilde{\\mathbb{V}}$ of the (non-unit-normalized) eigenvectors of the adjoint $\\mathbb{A}^\\dagger$ as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{\\widetilde{V}} = (\\mathbb{U}^{-1})^\\dagger.\\tag{14}\n",
    "\\end{align*}\n",
    "\n",
    "(The tilde is just to remind us that each of the column of $\\widetilde{\\mathbb{V}}$ is *not* normalized to have unit norm.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To understand why (14) is true, remember the orthogonality property between eigenvectors of $\\mathbb{A}$ and eigenvectors of $\\mathbb{A}^\\dagger$ that correspond to non-complex-conjugate eigenvalues. Thus:\n",
    "\n",
    "\\begin{align*}\n",
    "\\underbrace{\\begin{pmatrix} \\begin{bmatrix} & \\boldsymbol{\\upsilon}_1^* & \\end{bmatrix} \\\\ \\vdots \\\\ \\begin{bmatrix} & \\boldsymbol{\\upsilon}_n^* & \\end{bmatrix}\\end{pmatrix}}_{=\\mathbb{V}^{\\dagger}}\\,\\underbrace{\\begin{pmatrix} \\begin{bmatrix} \\\\ \\boldsymbol{u}_1 \\\\ \\\\ \\end{bmatrix} & \\dotsb & \\begin{bmatrix} \\\\ \\boldsymbol{u}_n \\\\\\\\ \\end{bmatrix}\\end{pmatrix}}_{=\\mathbb{U}} = \\underbrace{\\begin{pmatrix} \\alpha_1 & 0 & 0\\\\0 & \\ddots &0 \\\\ 0 & 0 & \\alpha_n\\end{pmatrix}}_{=\\mathbb{D}\\text{ diagonal matrix}}\n",
    "\\end{align*}\n",
    "\n",
    "with $\\alpha_j \\equiv (\\boldsymbol{\\upsilon}_j,\\boldsymbol{u}_j)$ the inner product between the eigenvectors of $\\mathbb{A}$ and the eigenvectors of $\\mathbb{A}^\\dagger$ with conjugate eigenvalue. From (..) it follows that\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{I} &= \\underbrace{\\begin{pmatrix} \\frac1{\\alpha_1^*}\\begin{bmatrix} & \\boldsymbol{\\upsilon}_1^* & \\end{bmatrix} \\\\ \\vdots \\\\ \\frac1{\\alpha_n^*}\\begin{bmatrix} & \\boldsymbol{\\upsilon}_n^* & \\end{bmatrix}\\end{pmatrix}}_{=\\left(\\mathbb{V}\\mathbb{D}^{-1}\\right)^{\\dagger}}\\,\\underbrace{\\begin{pmatrix} \\begin{bmatrix} \\\\ \\boldsymbol{u}_1 \\\\ \\\\ \\end{bmatrix} & \\dotsb & \\begin{bmatrix} \\\\ \\boldsymbol{u}_n \\\\\\\\ \\end{bmatrix}\\end{pmatrix}}_{=\\mathbb{U}}\n",
    "\\end{align*}\n",
    "\n",
    "From the above we conclude that\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{V}= (\\mathbb{U}^{-1})^\\dagger \\mathbb{D} \\tag{15}\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       V = Complex{Float64}[0.64+0.25im -0.51-0.45im; 0.72+0.00im 0.74+0.00im]\n",
      "U^(-1)*D = Complex{Float64}[0.64+0.25im -0.51-0.45im; 0.72-0.00im 0.74+0.00im]\n"
     ]
    }
   ],
   "source": [
    "A = rand(2, 2) + im*rand(2,2); \n",
    "\n",
    "S, U = eigen(A); u1 = U[:, 1]; u2 =U[:, 2]\n",
    "Sadj, V = eigen(copy(A')); v1 = V[:, 1]; v2 =V[:, 2]\n",
    "\n",
    "D = Diagonal([dot(u1, v1), dot(u2, v2)])\n",
    "\n",
    "println(\"       V = \", V)\n",
    "println(\"U^(-1)*D = \", inv(U)'*D)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But the diagonal matrix $\\mathbb{D}$ is only a scale factor for each column of $\\mathbb{V}$. If we are not necessarily interested in unit-normed eigenvectors of $\\mathbb{A}^\\dagger$ then we just have (14).\n",
    "\n",
    "Having established (14) or (15) we can proceed to show that for $t\\to\\infty$ the eigenvector of $e^{\\mathbb{A^\\dagger}t} e^{\\mathbb{A}t}$ with maximum eigenvalue tends to the eigenvector of $\\mathbb{A}^\\dagger$ corresponding to the eigenvalue with maximum real part.\n",
    "\n",
    "Consider the eigenanalysis of $e^{\\mathbb{A}t}$: \n",
    "\n",
    "\\begin{align*}\n",
    "e^{\\mathbb{A}t} &= \\mathbb{U} \\, \\underbrace{\\begin{pmatrix} e^{\\delta_1 t} & 0 & 0\\\\0 & \\ddots &0 \\\\ 0 & 0 & e^{\\delta_n t}\\end{pmatrix}}_{=\\mathbb{\\Delta}}\\, \\mathbb{U}^{-1}. \\tag{16}\n",
    "\\end{align*}\n",
    "\n",
    "**Exercise**: It's easy to show that $\\mathbb{A}$ and $e^{\\mathbb{A}t}$ commute and thus share the same eigenvectors. That is why in (16) the eigenvectors of $\\mathbb{A}$ appear. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In eigenanalysis (16) the eigenvalues are ordered with descending real part, i.e., $\\mathrm{Re}(e^{\\delta_1 t})\\ge\\dotsb\\ge\\mathrm{Re}(e^{\\delta_n t})$. Also, $\\mathbb{U}^{-1}=\\widetilde{\\mathbb{V}}^{\\dagger}$ (from (14)). We can rewrite then (16) as\n",
    "\n",
    "\\begin{align*}\n",
    "e^{\\mathbb{A}t} = \\underbrace{\\begin{pmatrix} \\begin{bmatrix} \\\\ \\boldsymbol{u}_1 \\\\ \\\\ \\end{bmatrix} & \\dotsb & \\begin{bmatrix} \\\\ \\boldsymbol{u}_n \\\\\\\\ \\end{bmatrix}\\end{pmatrix}}_{=\\mathbb{U}}\\, \\underbrace{\\begin{pmatrix} e^{\\delta_1 t} & 0 & 0\\\\0 & \\ddots &0 \\\\ 0 & 0 & e^{\\delta_n t}\\end{pmatrix}}_{=\\mathbb{\\Delta}}\\,\\underbrace{\\begin{pmatrix} \\frac1{\\alpha_1^*}\\begin{bmatrix} & \\boldsymbol{\\upsilon}_1^* & \\end{bmatrix} \\\\ \\vdots \\\\ \\frac1{\\alpha_n^*}\\begin{bmatrix} & \\boldsymbol{\\upsilon}_n^* & \\end{bmatrix}\\end{pmatrix}}_{=\\widetilde{\\mathbb{V}}^{\\dagger}}.\\tag{17}\n",
    "\\end{align*}\n",
    "\n",
    "The eigenvalues are ordered with descending real part, i.e., $\\mathrm{Re}(e^{\\delta_1 t})\\ge\\dotsb\\ge\\mathrm{Re}(e^{\\delta_n t})$. Thus, for $t\\to\\infty$ the leading eigenvalue dominates and thus we have that \n",
    "\n",
    "\\begin{align*}\n",
    "e^{\\mathbb{A}t} \\sim \\frac{e^{\\delta_1 t} }{\\alpha_1^*}\\underbrace{\\begin{bmatrix} \\\\ \\boldsymbol{u}_1 \\\\ \\\\ \\end{bmatrix}  \\begin{bmatrix} & \\boldsymbol{\\upsilon}_1^* & \\end{bmatrix}}_{\\text{this is a matrix}}.\\tag{18}\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Similarly,\n",
    "\n",
    "\\begin{align*}\n",
    "e^{\\mathbb{A}^\\dagger t} &= (\\mathbb{U}^{-1})^\\dagger \\,\\begin{pmatrix} e^{\\delta_1^* t} & 0 & 0\\\\0 & \\ddots &0 \\\\ 0 & 0 & e^{\\delta_n^* t}\\end{pmatrix}\\,\\mathbb{U}^\\dagger\\\\\n",
    "&= \\widetilde{\\mathbb{V}}\\,\\begin{pmatrix} e^{\\delta_1^* t} & 0 & 0\\\\0 & \\ddots &0 \\\\ 0 & 0 & e^{\\delta_n^* t}\\end{pmatrix}\\,\\mathbb{V}^{-1}.\\tag{19}\n",
    "\\end{align*}\n",
    "\n",
    "and for $t\\to \\infty$\n",
    "\n",
    "\\begin{align*}\n",
    "e^{\\mathbb{A}^\\dagger t} \\sim \\frac{e^{\\delta_1^* t}}{\\alpha_1} \\begin{bmatrix} \\\\ \\boldsymbol{\\upsilon}_1 \\\\ \\\\ \\end{bmatrix}  \\begin{bmatrix} & \\boldsymbol{u}_1^* & \\end{bmatrix}. \\tag{20}\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Using (18) and (20) we have that\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{E(t)}{E(0)} &= \\frac{\\big(\\boldsymbol{\\phi}_0\\,,\\,e^{\\mathbb{A^\\dagger}t} e^{\\mathbb{A}t}\\boldsymbol{\\phi}_0\\big)}{\\big(\\boldsymbol{\\phi}_0\\,,\\,\\boldsymbol{\\phi}_0\\big)}\\\\\n",
    "&\\approx \\frac{e^{2\\mathrm{Re}(\\delta_1)t}}{\\left|\\alpha_1\\right|^2}\\frac{\\big(\\boldsymbol{\\phi}_0\\,,\\,\\boldsymbol{\\upsilon}_1\\,\\boldsymbol{u}_1^\\dagger \\boldsymbol{u}_1 \\boldsymbol{\\upsilon}_1^\\dagger\\boldsymbol{u}_0\\big)}{\\big(\\boldsymbol{\\phi}_0\\,,\\,\\boldsymbol{\\phi}_0\\big)}\\\\\n",
    "&= \\frac{e^{2\\mathrm{Re}(\\delta_1)t}}{\\left|\\alpha_1\\right|^2}\\frac{\\big(\\boldsymbol{\\phi}_0\\,,\\,\\boldsymbol{\\upsilon}_1\\,\\boldsymbol{\\upsilon}_1^\\dagger\\,\\boldsymbol{u}_0\\big)}{\\big(\\boldsymbol{\\phi}_0\\,,\\,\\boldsymbol{\\phi}_0\\big)}\\quad\\text{since }(\\boldsymbol{u},\\boldsymbol{u})=\\boldsymbol{u}^\\dagger \\boldsymbol{u}=1\\\\\n",
    "&= \\frac{e^{2\\mathrm{Re}(\\delta_1)t}}{\\left|\\alpha_1\\right|^2}\\frac{\\big(\\boldsymbol{\\upsilon}_1^\\dagger\\boldsymbol{u}_0\\,,\\,\\boldsymbol{\\upsilon}_1^\\dagger\\,\\boldsymbol{u}_0\\big)}{\\big(\\boldsymbol{\\phi}_0\\,,\\,\\boldsymbol{\\phi}_0\\big)}\\\\\n",
    "&= \\frac{e^{2\\mathrm{Re}(\\delta_1)t}}{\\left|\\alpha_1\\right|^2}\\frac{\\big|\\big(\\boldsymbol{\\upsilon}_1,\\boldsymbol{\\phi}_0\\big)\\big|^2}{\\big(\\boldsymbol{\\phi}_0\\,,\\,\\boldsymbol{\\phi}_0\\big)}, \\tag{21}\n",
    "\\end{align*}\n",
    "\n",
    "which is obviously maximized when $\\boldsymbol{\\phi}_0$ is parallel with $\\boldsymbol{\\upsilon}_1$. In that case\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{E(t)}{E(0)} &\\sim \\frac{e^{2\\mathrm{Re}(\\delta_1)t}}{\\left|(\\boldsymbol{\\upsilon}_1,\\boldsymbol{u}_1)\\right|^2}. \\tag{22}\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "To understand what this means, consider equation (22) reduces to for the $2\\times2$ example we were doing in class. In that case we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{E(t)}{E(0)} &\\sim \\frac{e^{2\\mathrm{Re}(\\delta_1)t}}{\\sin^2\\theta}. \\tag{23}\n",
    "\\end{align*}\n",
    "\n",
    "where $\\theta$ is the angle between the two eigenvectors of $\\mathbb{A}$.\n",
    "\n",
    "Note that from (22) (or (23)) it is apparent that not only have we computed that the best initial condition for maximizing energy growth at $t\\to\\infty$ is the eigenvector $\\boldsymbol{\\upsilon}_1$ of the adjoint $\\mathbb{A}^\\dagger$, but we have also computed the growth that this initial condition would produce. We can see that the energy growth is larger by a factor of $1/\\sin^2\\theta$ if we start of with $\\boldsymbol{\\upsilon}_1$ rather than just starting off with the eigenvector $\\boldsymbol{u}_1$of operator $\\mathbb{A}$ itself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### References\n",
    "\n",
    "Farrell, B. F. and P. J. Ioannou (1996) \"Generalized stability theory. Part I: Autonomous operators.\" *J. Atmos. Sci.*, **53 (14)**, 2025-2040."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 0.7.0",
   "language": "julia",
   "name": "julia-0.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
